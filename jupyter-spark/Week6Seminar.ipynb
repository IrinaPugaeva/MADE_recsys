{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as spf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import lightfm\n",
    "import lightfm.data as ld\n",
    "import lightfm.evaluation as lv\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import optuna\n",
    "\n",
    "import tensorboardX as tb\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"dnikanorova\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = spark.read.json(\"/user/dnikanorova/data/random_100k.json\")\n",
    "\n",
    "data_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (\n",
    "    data_all\n",
    "        #.filter(spf.col(\"time\") > 0.5)\n",
    "        .select(\"user\", \"track\", \"time\")\n",
    "        .toPandas()\n",
    "        .drop_duplicates(subset=[\"user\", \"track\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positives = data[data[\"time\"] > 0.8].copy()\n",
    "positives[\"test\"] = np.random.random(len(positives)) >= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_counts = positives[~positives[\"test\"]].groupby(\"user\").size()\n",
    "users = set(user_counts[user_counts >= 5].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_counts = positives[~positives[\"test\"]].groupby(\"track\").size()\n",
    "tracks = set(track_counts[track_counts >= 5].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positives[~positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "test_data = positives[positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = ld.Dataset()\n",
    "dataset.fit(users, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions, _ = dataset.build_interactions(train_data[[\"user\", \"track\"]].itertuples(index=False, name=None))\n",
    "test_interactions, _ = dataset.build_interactions(test_data[[\"user\", \"track\"]].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(epochs=1, at=10, loss=\"warp\", no_components=30, learning_rate=0.01, max_sampled=10, user_alpha=0.0, item_alpha=0.0, threads=30, verbose=False):\n",
    "    model = lightfm.LightFM(\n",
    "        no_components=no_components,\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        max_sampled=max_sampled,\n",
    "        user_alpha=user_alpha,\n",
    "        item_alpha=user_alpha,\n",
    "    )\n",
    "\n",
    "    precisions_at = []\n",
    "    for epoch in range(epochs):\n",
    "        model = model.fit_partial(train_interactions, num_threads=threads)\n",
    "        precision_at = lv.precision_at_k(model, test_interactions, train_interactions=train_interactions, k=at, num_threads=threads)\n",
    "        if verbose:\n",
    "            print(f\"{epoch}:\\t{np.mean(precision_at)} +/- {ss.sem(precision_at) * 1.96}\")\n",
    "        precisions_at.append(np.mean(precision_at))\n",
    "            \n",
    "    return model, precisions_at\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"warp\", \"bpr\"])\n",
    "    no_components = trial.suggest_categorical(\"no_components\", [10, 30, 50])\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.001, 0.01])\n",
    "    max_sampled = trial.suggest_categorical(\"max_sampled\", [10, 20, 50, 100])\n",
    "    user_alpha = trial.suggest_categorical(\"user_alpha\", [0.0, 0.001, 0.01])\n",
    "    item_alpha = trial.suggest_categorical(\"item_alpha\", [0.0, 0.001, 0.01])\n",
    "    \n",
    "    model, precisions_at = fit_model(\n",
    "        epochs=1, \n",
    "        at=10,\n",
    "        loss=loss,\n",
    "        no_components=no_components, \n",
    "        learning_rate=learning_rate, \n",
    "        max_sampled=max_sampled, \n",
    "        user_alpha=user_alpha, \n",
    "        item_alpha=item_alpha,\n",
    "    )\n",
    "    \n",
    "    return precisions_at[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = {'loss': 'warp',\n",
    " 'no_components': 50,\n",
    " 'learning_rate': 0.01,\n",
    " 'max_sampled': 100,\n",
    " 'user_alpha': 0.0,\n",
    " 'item_alpha': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, precisions_at = fit_model(\n",
    "    epochs=100,\n",
    "    at=10,\n",
    "    loss=best_params[\"loss\"],\n",
    "    no_components=best_params[\"no_components\"], \n",
    "    learning_rate=best_params[\"learning_rate\"], \n",
    "    max_sampled=best_params[\"max_sampled\"],\n",
    "    user_alpha=best_params[\"user_alpha\"],\n",
    "    item_alpha=best_params[\"item_alpha\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = pl.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(precisions_at)), precisions_at)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save track embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases, embeddings = model.get_item_representations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.mapping()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_meta = pd.read_json(DATA_DIR + \"tracks.json\", lines=True)\n",
    "track_meta[\"dataset_index\"] = track_meta[\"track\"].map(lambda t: dataset.mapping()[2].get(t))\n",
    "track_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tracks = track_meta[pd.notnull(track_meta[\"dataset_index\"])].sort_values(\"dataset_index\")\n",
    "dataset_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tb.SummaryWriter(comment='msd_ligtfm_embeddings', log_dir=DATA_DIR + \"tb\")\n",
    "writer.add_embedding(embeddings, metadata=list(dataset_tracks[[\"artist\", \"title\"]].itertuples(index=False, name=None)), tag=\"lightfm\", metadata_header=[\"artist\", \"title\"])\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute top recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "max_tracks_from_same_artist = 5\n",
    "\n",
    "with open(DATA_DIR + f\"recommendations_{k}_{max_tracks_from_same_artist}.json\", \"w\") as rf:\n",
    "    for _, track in tqdm.tqdm(track_meta.iterrows()):\n",
    "        j = track[\"dataset_index\"]\n",
    "        \n",
    "        recommendations = []\n",
    "        if pd.notna(j):\n",
    "            embedding = embeddings[int(j)]\n",
    "            neighbours = np.argsort(-np.dot(embeddings, embedding))\n",
    "            \n",
    "            artists = defaultdict(int)\n",
    "            for neighbour in neighbours:\n",
    "                recommended_track = dataset_tracks[dataset_tracks[\"dataset_index\"] == neighbour].iloc[0]\n",
    "                \n",
    "                recommendation = int(recommended_track[\"track\"])\n",
    "                if recommendation == track[\"track\"]:\n",
    "                    continue\n",
    "                \n",
    "                artist = recommended_track[\"artist\"]\n",
    "                if artists[artist] >= max_tracks_from_same_artist:\n",
    "                    continue\n",
    "                \n",
    "                recommendations.append(recommendation)\n",
    "                artists[artist] += 1\n",
    "\n",
    "                if len(recommendations) == k:\n",
    "                    break\n",
    "         \n",
    "        track_with_recommendations = dict(track)\n",
    "        track_with_recommendations[\"recommendations\"] = recommendations\n",
    "        \n",
    "        rf.write(json.dumps(track_with_recommendations) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique artist per recommendation list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_track_artists = dict(zip(\n",
    "    dataset_tracks[\"track\"].values.tolist(),\n",
    "    dataset_tracks[\"artist\"].values.tolist(),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_json(DATA_DIR + f\"recommendations_{k}_{max_tracks_from_same_artist}.json\", lines=True)\n",
    "recs = recs[recs[\"dataset_index\"].notnull()]\n",
    "\n",
    "sample = recs.sample(frac=0.1).iloc[0]\n",
    "\n",
    "print(sample[\"title\"], \"by\" , sample[\"artist\"], \"\\n===\")\n",
    "print(\"\\n\".join([dataset_track_artists[track] for track in sample[\"recommendations\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_artists(tracks):\n",
    "    return len(\n",
    "        set([dataset_track_artists[track] for track in tracks])\n",
    "    )\n",
    "\n",
    "\n",
    "recs[\"artist_counts\"] = recs[\"recommendations\"].map(count_artists)\n",
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[\"artist_counts\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atp-mobod_2022",
   "language": "python",
   "name": "atp-mobod_2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
